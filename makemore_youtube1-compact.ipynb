{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  makemore demo - YouTube follow along\n",
    "\n",
    "[Video - The spelled-out intro to language modeling: building makemore](https://www.youtube.com/watch?v=PaCmpygFfXo&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup - common to both approaches \n",
    "import torch\n",
    "\n",
    "# data set: 32k first names\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "\n",
    "# s to i lookup, setting `.` as 0 index in array and all others + 1\n",
    "# we'll use `.` to mark the start and end of all words\n",
    "stoi = {s: i+1 for i, s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "\n",
    "# i to s lookup\n",
    "itos = {i: s for s, i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 1: non-neural network approach: count frequency of bigrams and store in `N`\n",
    "#\n",
    "# Create a 27x27 matrix with values all set to 0\n",
    "N = torch.zeros((27, 27), dtype=torch.int32)\n",
    "\n",
    "# Get the counts\n",
    "for w in words:\n",
    "  # use `.` to mark the start and end of all words\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2 in zip(chs, chs[1:]):\n",
    "    # integer index of this character in stoi 0-27\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    N[ix1, ix2] += 1\n",
    "\n",
    "# prepare probabilities, parameters of our bigram language model\n",
    "# Apply \"model smoothing\" using `N + 1` instead of `N`. This prevents 0's in probability matrix P, which could lead to `infinity` for loss measurement.\n",
    "P = (N + 1).float()\n",
    "# 27, 27\n",
    "# 27, 1  # This is \"broadcastable\" and it stretches the 1 into all 27 rows\n",
    "# https://pytorch.org/docs/stable/notes/broadcasting.html?highlight=broadcasting\n",
    "\n",
    "# Below uses `/=` to avoid creating new tensor, ie more efficient\n",
    "P /= P.sum(1, keepdim=True)\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mor.\n",
      "axx.\n",
      "minaymoryles.\n",
      "kondlaisah.\n",
      "anchshizarie.\n"
     ]
    }
   ],
   "source": [
    "# Sample\n",
    "for i in range(5):\n",
    "  out = []\n",
    "  ix = 0\n",
    "  while True:\n",
    "    p = P[ix]\n",
    "    ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "    out.append(itos[ix])\n",
    "    # Break with `.` is found, marking the end of the word\n",
    "    if ix == 0:\n",
    "      break\n",
    "\n",
    "  print(''.join(out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples:  228146\n"
     ]
    }
   ],
   "source": [
    "# Approach 2: neural network approach trained on bigrams\n",
    "\n",
    "# for one hot encoding: `F.one_hot` below\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#\n",
    "# Dataset: 228K bigrams from the 32K example names\n",
    "#\n",
    "xs, ys = [], []\n",
    "for w in words:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2 in zip(chs, chs[1:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    xs.append(ix1)\n",
    "    ys.append(ix2)\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.nelement()\n",
    "print('number of examples: ', num)\n",
    "\n",
    "# initialize the 'network'\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4899590015411377\n",
      "2.4897918701171875\n",
      "2.489628314971924\n",
      "2.489469528198242\n",
      "2.489314079284668\n",
      "2.4891624450683594\n",
      "2.4890148639678955\n",
      "2.48887038230896\n",
      "2.488729476928711\n",
      "2.4885923862457275\n",
      "2.4884581565856934\n",
      "2.4883270263671875\n",
      "2.4881982803344727\n",
      "2.4880735874176025\n",
      "2.4879510402679443\n",
      "2.4878318309783936\n",
      "2.487715005874634\n",
      "2.487600564956665\n",
      "2.4874887466430664\n",
      "2.487379550933838\n",
      "2.4872729778289795\n",
      "2.487168550491333\n",
      "2.4870662689208984\n",
      "2.486966133117676\n",
      "2.4868686199188232\n",
      "2.4867727756500244\n",
      "2.4866793155670166\n",
      "2.4865870475769043\n",
      "2.486497640609741\n",
      "2.4864094257354736\n",
      "2.486323118209839\n",
      "2.486238718032837\n",
      "2.4861562252044678\n",
      "2.4860751628875732\n",
      "2.4859957695007324\n",
      "2.4859180450439453\n",
      "2.485841989517212\n",
      "2.485767126083374\n",
      "2.4856936931610107\n",
      "2.485621929168701\n",
      "2.4855518341064453\n",
      "2.4854824542999268\n",
      "2.485414743423462\n",
      "2.4853484630584717\n",
      "2.485283374786377\n",
      "2.4852190017700195\n",
      "2.485156536102295\n",
      "2.4850947856903076\n",
      "2.485034465789795\n",
      "2.4849750995635986\n",
      "2.484917163848877\n",
      "2.4848599433898926\n",
      "2.4848034381866455\n",
      "2.4847488403320312\n",
      "2.484694719314575\n",
      "2.4846413135528564\n",
      "2.484589099884033\n",
      "2.4845378398895264\n",
      "2.4844870567321777\n",
      "2.4844377040863037\n",
      "2.484389066696167\n",
      "2.4843411445617676\n",
      "2.4842944145202637\n",
      "2.484247922897339\n",
      "2.4842023849487305\n",
      "2.4841578006744385\n",
      "2.484114170074463\n",
      "2.4840705394744873\n",
      "2.4840285778045654\n",
      "2.4839868545532227\n",
      "2.483945846557617\n",
      "2.48390531539917\n",
      "2.4838650226593018\n",
      "2.4838263988494873\n",
      "2.483787775039673\n",
      "2.4837496280670166\n",
      "2.483712911605835\n",
      "2.483675956726074\n",
      "2.48363995552063\n",
      "2.4836041927337646\n",
      "2.483569383621216\n",
      "2.483535051345825\n",
      "2.4835007190704346\n",
      "2.4834675788879395\n",
      "2.4834346771240234\n",
      "2.4834022521972656\n",
      "2.483370304107666\n",
      "2.4833388328552246\n",
      "2.4833080768585205\n",
      "2.4832773208618164\n",
      "2.4832472801208496\n",
      "2.483217716217041\n",
      "2.4831883907318115\n",
      "2.4831597805023193\n",
      "2.4831314086914062\n",
      "2.4831035137176514\n",
      "2.4830760955810547\n",
      "2.483048439025879\n",
      "2.4830219745635986\n",
      "2.4829957485198975\n"
     ]
    }
   ],
   "source": [
    "# Gradient descent\n",
    "for k in range(100):\n",
    "\n",
    "  # forward pass\n",
    "  # input to the network: one-hot encoding\n",
    "  xenc = F.one_hot(xs, num_classes=27).float()\n",
    "  logits = xenc @ W  # predict log-counts\n",
    "  counts = logits.exp()  # counts, equivalent to N\n",
    "  # probabilities for next character\n",
    "  probs = counts / counts.sum(1, keepdims=True)\n",
    "  # regularization loss: `0.01*(W**2).mean()` tries to make all W's 0\n",
    "  # if `0.01` is higher it will be more uniform and not\n",
    "  loss = -probs[torch.arange(num), ys].log().mean() + 0.01 * (W**2).mean()\n",
    "  print(loss.item())\n",
    "\n",
    "  # backward pass\n",
    "  W.grad = None  # set to zero the gradient\n",
    "  loss.backward()\n",
    "\n",
    "  # update\n",
    "  W.data += -50 * W.grad\n",
    "\n",
    "# Earlier we had 2.47 loss when we manually did the counts.\n",
    "# So we'd like this neural network approach to become as \"good\", when measuring the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mor.\n",
      "axx.\n",
      "minaymoryles.\n",
      "kondmaisah.\n",
      "anchshizarie.\n"
     ]
    }
   ],
   "source": [
    "# Sample from neural net model\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(5):\n",
    "\n",
    "  out = []\n",
    "  ix = 0\n",
    "  while True:\n",
    "\n",
    "    xenc = F.one_hot(torch.tensor([ix]), num_classes=27).float()\n",
    "    logits = xenc @ W  # predict log-counts\n",
    "    counts = logits.exp()  # counts, equivalent to N\n",
    "    p = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "\n",
    "    ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "    out.append(itos[ix])\n",
    "    if ix == 0:\n",
    "      break\n",
    "  print(''.join(out))\n",
    "\n",
    "  # Gives the exact same output as the original P matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mor.\n",
    "axx.\n",
    "minaymoryles.\n",
    "kondlaisah.\n",
    "anchshizarie.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 64-bit ('3.9.15')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6dbec04a431f157b103533e84207c2f49dc4e813632efce31ad46431e6a7d537"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
